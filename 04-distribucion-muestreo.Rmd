# Estimaci칩n y distribuci칩n de muestreo 

```{r setup, include=FALSE, message=FALSE}
library(tidyverse)
source("R/funciones_auxiliares.R")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE, fig.align = 'center',fig.width = 5, fig.height=3)
comma <- function(x) format(x, digits = 2, big.mark = ",")
theme_set(theme_minimal())
```

En esta secci칩n discutiremos cu치l el objetivo general del proceso de estimaci칩n,
y c칩mo entender y manejar la variabilidad que se produce cuando aleatorizamos
la selecci칩n de las muestras que utilizamos para hacer an치lisis.

A diferencia de las pruebas de permutaci칩n, donde evalu치bamos como cambiar칤a una 
estad칤sitica si un tratamiento o grupo se hubiera asignado de forma distinta, en la
siguiente secci칩n nos preguntamos como var칤a una estad칤stica entre muestras. Por ejemplo, 
pasaremos de preguntar si una vacuna reduce el riesgo de una enfermedad a evaluar 
en que magnitud se reduce el riesgo de contraer la enfermedad.

<!-- 
Unlike randomization tests (which modeled how the statistic would change if the treatment had been allocated differently), the bootstrap will model how a statistic varies from one sample to another taken from the population.  Mine https://openintro-ims.netlify.app/foundations-bootstrapping.html-->

## Ejemplo: precios de casas {-}

Supongamos que queremos conocer el valor total de las casas
que se vendieron recientemente en una zona
particular. Supondremos que tenemos un listado de las casas que se han
vendido recientemente, pero en ese listado no se encuentra el precio de venta.
Decidimos entonces tomar una muestra aleatoria de 100 de esas casas. Para esas
casas hacemos trabajo de campo para averiguar el precio de venta.

```{r}
marco_casas <- read_csv("data/casas.csv")
set.seed(841)
muestra_casas <- sample_n(marco_casas, 100) |>
  select(id, nombre_zona, area_habitable_sup_m2, precio_miles)
sprintf("Hay %0.0f casas en total, tomamos muestra de %0.0f",
        nrow(marco_casas), nrow(muestra_casas))
head(muestra_casas)
```

Como tomamos una muestra aleatoria, intentamos estimar el valor
total de las casas que se vendieron expandiendo el total muestral, es decir
nuestro estimador $\hat{t} = t(X_1,\ldots X_{100})$ del total
poblacional $t$ es

$$\hat{t} = \frac{N}{n} \sum_{i=1}^{100} X_i = N\bar{x}$$

Esta funci칩n implementa el estimador:

```{r}
n <- nrow(muestra_casas) # tama침o muestra
N <- nrow(marco_casas) # tama침o poblaci칩n
estimar_total <- function(muestra_casas, N){
  total_muestral <- sum(muestra_casas$precio_miles)
  n <- nrow(muestra_casas)
  # cada unidad de la muestra representa a N/n
  f_exp <- N / n
  # estimador total es la expansi칩n del total muestral
  estimador_total <- f_exp * total_muestral
  res <- tibble(total_muestra = total_muestral,
         factor_exp = f_exp,
         est_total_millones = estimador_total / 1000)
  res
}
estimar_total(muestra_casas, N) |>
  mutate(across(where(is.numeric), \(x) round(x, 2)))
```

Sin embargo, si hubi칠ramos obtenido otra muestra, hubi칠ramos obtenido otra
estimaci칩n diferente. Por ejemplo:

```{r}
estimar_total(sample_n(marco_casas, 100), N) |>
  mutate(across(where(is.numeric), \(x) round(x, 2)))
```

El valor poblacional que buscamos estimar (n칩tese que en la pr치ctica este no lo conocemos)
es:

```{r}
# multiplicar por 1000 para que sea en millones de d칩lares
total_pob <- sum(marco_casas |> pull(precio_miles)) / 1000
total_pob
```

As칤 que:

- Para algunas muestras esta estad칤stica puede estar muy cercana al valor poblacional,
pero para otras puede estar m치s lejana.
- Para entender qu칠 tan buena es una estimaci칩n
particular, entonces, tenemos que entender *cu치nta variabilidad hay de muestra a muestra*
debida a la aleatorizaci칩n. Esto depende del dise침o de la muestra y
de la poblaci칩n de precios de casas (que no conocemos).

## Distribuci칩n de muestreo {-}

La distribuci칩n de muestreo de una estad칤stica enumera los posibles resultados
que puede tomar esa estad칤stica sobre todas las muestras posibles. Este es el concepto
b치sico para poder entender qu칠 tan bien o mal estima un par치metro poblacional dado.

En nuestro ejemplo anterior de precio de casas, no podemos calcular todas las posibles
estimaciones bajo todas las posibles muestras, pero podemos aproximar
repitiendo una gran cantidad de veces el proceso de muestreo, como hicimos
al aproximar la distribuci칩n de permutaciones de estad칤sticas de prueba de las
secciones anteriores.

Empezamos repitiendo 10 veces y examinamos c칩mo var칤a nuestra estad칤stica:

```{r}
replicar_muestreo <- function(marco_casas, m = 500, n){
  # n es el tama침o de muestra que se saca de marco_casas
  # m es el n칰mero de veces que repetimos el muestro de tama침o n
  resultados <- map_df(1:m,
      function(id) {
        sample_n(marco_casas, n) |>
          estimar_total(N) 
      }, .id = "id_muestra")
}
replicar_muestreo(marco_casas, m = 10, n = 100) |>
  mutate(across(where(is.numeric), round, 1)) |>
  formatear_tabla()
```

Como vemos, hay variaci칩n considerable en nuestro estimador del total, pero
la estimaci칩n que har칤amos con cualquiera de estas muestras no es muy mala. Ahora
examinamos un n칰mero m치s grande de simulaciones:

```{r, cache = TRUE}
replicaciones_1 <- replicar_muestreo(marco_casas, m = 1500, n = 100)
```

Y el siguiente histograma nos dice qu칠 podemos esperar de la variaci칩n de
nuestras estimaciones, y donde es m치s probable que una estimaci칩n particular caiga:

```{r}
graf_1 <- ggplot(replicaciones_1, aes(x = est_total_millones)) +
  geom_histogram() +
  geom_vline(xintercept = total_pob, colour = "red") +
  xlab("Millones de d칩lares") +
  scale_x_continuous(breaks = seq(180, 240, 10), limits = c(180, 240))
graf_1
```

Con muy alta probabilidad  el error no ser치 de m치s de unos 30 millones de d칩lares
(o no m치s de 20% del valor poblacional).


```{block, type='mathblock'}
**Definici칩n** Sea $X_1, X_2, \ldots X_n$ una muestra,
y $T = t(X_1, X_2, \ldots, X_n)$ una estad칤stica.

La **distribuci칩n de muestreo** de $T$ es la funci칩n de distribuci칩n de $T$. Esta
distribuci칩n es sobre todas las posibles muestras que se pueden obtener.

Cuando usamos $T$ para estimar alg칰n par치metro poblacional $\theta$, decimos
informalmente que el estimador es **preciso** si su distribuci칩n de muestreo est치
muy concentrada alrededor del valor $\theta$ que queremos estimar.
```

Si la distribuci칩n de muestreo est치 concentrada en un conjunto muy grande
o muy disperso, quiere decir que con alta probabilidad cuando obtengamos
nuestra muestra y calculemos nuestra estimaci칩n, el resultado estar치 lejano
del valor poblacional que nos interesa estimar.

Veamos qu칠 pasa cuando hacemos la muestra m치s grande en nuestro ejemplo:

```{r, cache = TRUE}
replicaciones_2 <- replicar_muestreo(marco_casas, m = 1500, n = 250)
```

Graficamos las dos distribuciones de muestreo juntas, y vemos c칩mo
con mayor muestra obtenemos un estimador m치s preciso, y sin considerar el costo,
preferimos el estimador **mejor concentrado alrededor del valor que buscamos estimar**.

```{r, fig.width = 8, fig.height = 4}
library(patchwork)
graf_2 <- ggplot(replicaciones_2, aes(x = est_total_millones)) +
  geom_histogram() +
  geom_vline(xintercept = total_pob, colour = "red") +
  xlab("Millones de d칩lares") +
  scale_x_continuous(breaks = seq(180, 240, 10), limits = c(180, 240))
graf_1 + graf_2
```

```{block, type = 'comentario'}
**Observaci칩n**: a veces este concepto se confunde la distribuci칩n
poblacional de las $X_i$. Esto es muy diferente. 

```

Por ejemplo, en nuestro caso, el histograma de la distribuci칩n de valores poblacionales es

```{r, fig.width = 5, fig.height=3}
ggplot(marco_casas, aes(x = precio_miles)) + geom_histogram()
```
que en general no tiene ver mucho en escala o forma con la distribuci칩n de muestreo
de nuestro estimador del total.

## M치s ejemplos {-}

Podemos tambi칠n considerar muestrear de poblaciones sint칠ticas o modelos
probabil칤sticos que usamos para modelar poblaciones reales.

Por ejemplo, supongamos que tomamos una muestra de tama침o 15 de la distribuci칩n
uniforme en $[0,1]$. Es decir, cada $X_i$ es un valor uniformemente distribuido
en $[0,1]$, y las $X_i$ se extraen independientemente unas de otras. Consideramos
dos estad칤sticas de inter칠s:

1. La media muestral $T_1(X) = \frac{1}{15}\sum_{i = 1}^{15} X_i$
2. El cuantil 0.75 de la muestra $T_2(X) = q_{0.75}(X)$

```{block2, type='ejercicio'}
쮺칩mo crees que se vean las distribuciones muestrales de estas estad칤sticas?
  쮸lrededor de qu칠 valores crees que concentren? 쮺rees que tendr치n mucha o poca
dispersi칩n? 쯈u칠 forma crees que tengan?
```

Para el primer caso hacemos:

```{r}
# simular
replicar_muestreo_unif <- function(est = mean, m, n = 15){
  valores_est <- map_dbl(1:m, ~ est(runif(n)))
  tibble(id_muestra = 1:m, estimacion = valores_est)
}
sim_estimador_1 <- replicar_muestreo_unif(mean, 4000, 15)
# graficar aprox de distribuci칩n de muestreo
ggplot(sim_estimador_1, aes(x = estimacion)) +
  geom_histogram(bins = 40) +
  xlim(c(0, 1))
```

```{r}
# simular para el m치ximo
cuantil_75 <- function(x) quantile(x, 0.75)
sim_estimador_2 <- replicar_muestreo_unif(cuantil_75, 4000, 15)
# graficar distribuci칩n de muestreo
ggplot(sim_estimador_2, aes(x = estimacion)) +
  geom_histogram(breaks = seq(0, 1, 0.02)) +
  xlim(c(0, 1))
```


```{block2, type='ejercicio'}
Sup칩n que tenemos una muestra de 30 observaciones de una distribuci칩n
uniforme $[0,b]$.

- 쯈u칠 tan buen estimador de $b/2$ es la media muestral? 쮺칩mo lo cuantificar칤as?
- 쯈u칠 tan buen estimador del cuantil 0.8 de la distribuci칩n uniforme es
el cuantil 0.8 muestral? 쯈u칠 desventajas notas en este estimador?

```

## El error est치ndar {-}

Una primera medida 칰til de la dispersi칩n de la distribuci칩n de muestreo
es su desviaci칩n est치ndar: la raz칩n espec칤fica tiene qu칠 ver
con un resultado importante, el teorema central del l칤mite, que veremos
m치s adelante. En este caso particular, a esta desviaci칩n est치ndar
se le llama error est치ndar:

```{block, type='mathblock'}
**Definici칩n** A la desviaci칩n est치ndar de una estad칤stica $T$ le llamamos
su **error est치ndar**, y la denotamos por $\text{ee}(T)$. A cualquier estimador
de este error est치ndar lo denotamos como $\hat{\text{ee}}(T)$.

```

Este error est치ndar mide qu칠 tanto var칤a el estimador $T$ de muestra a muestra.

```{block, 'comentario'}
**Observaci칩n**: es importante no confundir el error est치ndar con
la desviaci칩n est치ndar de una muestra (o de la poblaci칩n).
```


En nuestro ejemplo
de las uniformes, la desviaci칩n est치ndar de las muestras var칤a como:

```{r}
map_dbl(1:10000, ~ sd(runif(15))) |> quantile() |> round(2)
```

Mientras que el error est치ndar de la media es aproximadamente

```{r}
map_dbl(1:10000, ~ mean(runif(15))) |> sd()
```

y el error est치ndar del m치ximo es aproximadamente

```{r}
map_dbl(1:10000, ~ max(runif(15))) |> sd()
```

```{block2, type='ejercicio'}
Como ejercicio para contrastar estos conceptos,
puedes considerar: 쯈u칠 pasa con la desviaci칩n est치ndar de una muestra muy 
grande de uniformes? 쯈u칠 pasa con el error est치ndar de la media muestral de una muestra muy grande de uniformes?
```



### Ejemplo: valor de casas {-}

Consideramos el error est치ndar del estimador del total del inventario vendido, usando
una muestra de 250 con el estimador del total que describimos arriba. Como aproximamos con
simulaci칩n la distribuci칩n de muestreo, podemos hacer:

```{r}
ee_2 <- replicaciones_2 |> pull(est_total_millones) |> sd()
round(ee_2, 1)
```

que est치 en millones de pesos y cuantifica la dispersi칩n de la distribuci칩n de
muestreo del estimador del total.

Para tama침o de muestra 100, obtenemos m치s dispersi칩n:

```{r}
ee_1 <- replicaciones_1 |> pull(est_total_millones) |> sd()
round(ee_1, 1)
```

N칩tese que esto es muy diferente, por ejemplo, a la desviaci칩n est치ndar
poblacional o de una muestra. Estas dos cantidades miden la variabilidad del
estimador del total.

## Calculando la distribuci칩n de muestreo {-}

En los ejemplos anteriores usamos simulaci칩n para obtener aproximaciones
de la distribuci칩n de muestreo de algunos estimadores. Tambi칠n
es posible:

- Hacer c치lculos exactos a partir de modelos
probabil칤sticos.
- Hacer aproximaciones asint칩ticas para muestras grandes (de las cuales
la m치s importante es la que da el teorema central del l칤mite).

En los ejemplos de arriba, cuando muestreamos de la poblaciones,
extrajimos las muestras de manera aproximadamente independiente. Cada
observaci칩n $X_i$ tiene la misma distribuci칩n y las $X_i$'s son
independientes. Este tipo de dise침os aleatorizados es de los m치s
simples, y  se llama **muestreo aleatorio simple**.

En general, en esta parte haremos siempre este supuesto: Una **muestra**
es iid (independiente e id칠nticamente distribuida) si es
es un conjunto de observaciones $X_1,X_2, \ldots X_n$ independientes,
y cada una con la misma distribuci칩n.

En t칠rminos de poblaciones, esto lo logramos obteniendo cada observaci칩n
de manera aleatoria con el mismo procedimiento. En t칠rminos de modelos
probabil칤sticos, cada $X_i$ se extrae de la misma distribuci칩n fija $F$
(que pensamos como la "poblaci칩n") de manera independiente. Esto lo denotamos
por $X_i \overset{iid}{\sim} F.$

### Ejemplo {-}

Si $X_1, X_2, \ldots X_n$ es una muestra de uniformes independientes en $[0,1]$, 쯖칩mo
calcular칤amos la distribuci칩n de muestreo del m치ximo muestra $T_2 = \max$? En este
caso, es f치cil calcular su funci칩n de distribuci칩n acumulada de manera exacta:

$$F_{\max}(x) = P(\max\{X_1,X_2,\ldots X_n\} \leq x)$$
El m치ximo es menor o igual a $x$ si y s칩lo si todas las $X_i$ son menores
o iguales a $x$, as칤 que
$$F_{\max} (x) = P(X_1\leq x, X_2\leq x, \cdots, X_n\leq x)$$
como las $X_i$'s son independientes entonces
$$F_{\max}(x) = P(X_1\leq x)P(X_2\leq x)\cdots P(X_n\leq x) = x^n$$
para $x\in [0,1]$, pues para cada $X_i$ tenemos $P(X_i\leq x) = x$.
As칤 que no es necesario usar simulaci칩n para conocer esta distribuci칩n de muestreo.
Derivando esta distribuci칩n acumulada obtenemos su densidad, que es

$$f(x) = nx^{n-1}$$

para $x\in [0,1]$, y es cero en otro caso.

Si comparamos con nuestra simulaci칩n:

```{r}
teorica <- tibble(x = seq(0, 1 ,0.001)) |>
  mutate(f_dens = 15 * x^14)
sim_estimador_3 <- replicar_muestreo_unif(max, 4000, 15)
ggplot(sim_estimador_3) +
  geom_histogram(aes(x = estimacion), breaks = seq(0, 1, 0.02)) +
  xlim(c(0.5, 1)) +
  # el histograma es de ancho 0.02 y el n칰mero de simulaciones 4000  
  geom_line(data = teorica, aes(x = x, y = (4000 * 0.02) * f_dens),
            colour = "red", linewidth = 1.3)
```

Y vemos que con la simulaci칩n obtuvimos una buena aproximaci칩n


**Nota**: 쯖칩mo se relaciona un histograma con la funci칩n de densidad
que genera los datos? Sup칩n que $f(x)$ es una funci칩n de densidad, y
obtenemos un n칰mero $n$ de simulaciones independientes. Si escogemos
un histograma de ancho $\Delta$, 쯖u치ntas observaciones esperamos
que caigan en un intervalo $I = [a - \Delta/2, a + \Delta/2]$?. La probabilidad
de que una observaci칩n caiga en $I$ es igual a

$$P(X\in I) = \int_I f(x)\,dx = \int_{a - \Delta/2}^{a + \Delta/2} f(x)\,dx \approx f(a) \text{long}(I) = f(a) \Delta$$
para $\Delta$ chica. Si nuestra muestra es de tama침o $n$, el n칰mero esperado
de observaciones que caen en $I$ es entonces $nf(a)\Delta$. Eso explica
el ajuste que hicimos en la gr치fica de arriba. Otra manera de hacer es
ajustando el histograma: si en un intervalo el histograma alcanza el valor $y$,
$$f(a) = \frac{y}{n\Delta}$$

```{r}
teorica <- tibble(x = seq(0, 1 ,0.001)) |>
  mutate(f_dens = 15*x^{14})
ggplot(sim_estimador_3) +
  geom_histogram(aes(x = estimacion, y = after_stat(density)), breaks = seq(0, 1, 0.02)) +
  xlim(c(0.5, 1)) +
  # el histograma es de ancho 0.02 y el n칰mero de simulaciones 4000  
  geom_line(data = teorica, aes(x = x, y = f_dens),
            colour = "red", size = 1.3)
```

### Ejemplo {-}

Supongamos que las $X_i$'s son independientes y exponenciales con tasa $\lambda > 0$.
쮺u치l es la distribuci칩n de muestreo de la suma $S = X_1 + \cdots + X_n$? Sabemos que la suma
de exponenciales independientes es una distribuci칩n gamma con par치metros $(n, \lambda)$,
y esta es la distribuci칩n de muestreo de nuestra estad칤stica $S$ bajo las hip칩tesis
que hicimos.

Podemos checar este resultado con simulaci칩n, por ejemplo para una
muestra de tama침o $n=15$ con $\lambda = 1$:

```{r}
replicar_muestreo_exp <- function(est = mean, m, n = 150, lambda = 1){
  valores_est <- map_dbl(1:m, ~ est(rexp(n, lambda)))
  tibble(id_muestra = 1:m, estimacion = valores_est)
}
sim_estimador_1 <- replicar_muestreo_exp(sum, 4000, n = 15)
teorica <- tibble(x = seq(0, 35, 0.001)) |>
  mutate(f_dens = dgamma(x, shape = 15, rate = 1))
# graficar aprox de distribuci칩n de muestreo
ggplot(sim_estimador_1) +
  geom_histogram(aes(x = estimacion, y = after_stat(density)), bins = 35) +
  geom_line(data = teorica, aes(x = x, y = f_dens), colour = "red", linewidth = 1.2)
```


## Teorema central del l칤mite {-}

Si consideramos los ejemplos de arriba donde tratamos con estimadores
basados en una suma, total o una media ---y en menor medida cuantiles muestrales---,
vimos que las distribuci칩n de
muestreo de las estad칤sticas que usamos tienden a tener una forma com칰n.
Estas son manifestaciones de una regularidad estad칤stica importante que
se conoce como el **teorema central del l칤mite**: las distribuciones de muestreo
de sumas y promedios son aproximadamente normales cuando el tama침o de muestra
es suficientemente grande.

```{block2, type="mathblock"}
**Teorema central del l칤mite**
 
  Si $X_1,X_2, \ldots, X_n$ son independientes e id칠nticamente distribuidas con
media $\mu$ y desviaci칩n est치ndar $\sigma$ finitas.

Si el tama침o de muestra $n$ es grande,  entonces la distribuci칩n de muestreo de la media 

$$\bar{X} = \frac{X_1 + X_2 +\cdots + X_n}{n}$$ 
  
  es aproximadamente normal con media $\mu$ y desviaci칩n est치ndar $\sigma/\sqrt{n}$,
que escribimos como

$$\bar{X} \xrightarrow{} \mathsf{N}\left( \mu, \frac{\sigma}{\sqrt{n}} \right)$$

Adicionalmente, la distribuci칩n de la
media estandarizada converge a una distribuci칩n normal
est치ndar cuando $n$ es grande:
$$\sqrt{n} \, \left( \frac{\bar{X}-\mu}{\sigma} \right) \xrightarrow{}  \mathsf{N}(0, 1)$$

```

- El error est치ndar de $\bar{X}$ es
$\text{ee}(\bar{X}) = \frac{\sigma}{\sqrt{n}}$. Si tenemos una muestra, podemos
estimar $\sigma$ con de la siguiente forma:
$$\hat{\sigma} =\sqrt{\frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2}$$
o el m치s com칰n (que explicaremos m치s adelante)
$$\hat{s} = \sqrt{\frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2}$$

- Este hecho junto con el teorema del l칤mite central nos dice cu치l es la dispersi칩n,
y c칩mo se distribuyen las posibles desviaciones de la media muestral alrededor
de la verdadera media poblacional.

- 쯈u칠 tan grande debe ser $n$. Depende de c칩mo es la poblaci칩n. Cuando la poblaci칩n
tiene una distribuci칩n muy sesgada, por ejemplo, $n$ t칤picamente
necesita ser m치s grande que cuando la poblaci칩n es sim칠trica si queremos
obtener una aproximaci칩n "buena".

- En algunos textos se afirma que $n\geq 30$ es suficiente para que la
aproximaci칩n del Teorema central del l칤mite (TCL) sea buena siempre y cuando
la distribuci칩n poblacional no sea muy sesgada. Esta regla es m치s o menos arbitraria
y es mejor no confiarse, pues f치cilmente puede fallar. En la pr치ctica es importante
checar este supuesto, por ejemplo usando remuestreo (que veremos m치s adelante)

```{block, type='ejercicio'}
Revisa los ejemplos que hemos visto hasta ahora (precios de casas, simulaciones
de uniformes y exponenciales seg칰n las distintas estad칤sticas que consideramos).
쯈u칠 distribuciones de muestreo
parecen tener una distribuci칩n normal? 쮺칩mo juzgamos si estas distribuciones est치n
cerca o lejos de una distribuci칩n normal?
```

## Normalidad y gr치ficas de cuantiles normales {-}

Para checar si una distribuci칩n de datos dada es similar a la normal, la herramienta
mas com칰n en est치d칤stica es la gr치fica de cuantiles te칩ricos, que es una generalizaci칩n
de la gr치fica de cuantiles que vimos anteriormente. 

En primer lugar, definimos la funci칩n de cuantiles de una distribuci칩n te칩rica,
que es an치loga a la que definimos para conjuntos de datos:

Supongamos que tenemos una distribuci칩n acumulada te칩rica $\Phi$. Podemos
definir el cuantil-$f$ $q(f)$ de $\Phi$  como
el valor $q(f)$ tal que
$$q(f) = \text{argmin}\{x \,| \, \Phi(x)\geq f \}$$

En el caso de que $\Phi$ tiene densidad $\phi$, y su soporte es un intervalo (que puede
ser de longitud infinita), entonces podemos tambi칠n escribir $q(f)$
como el valor 칰nico donde acumulamos $f$ de la probabilidad

$$\int_{-\infty}^{q(f)} \phi(x)\,dx= f$$
Por ejemplo, para una densidad normal, abajo mostramos los cuantiles $f=0.5$ (mediana)
y $f=0.95$

```{r}
densidad_tbl <- tibble(x = seq(0, 10, 0.01)) |> 
  mutate(densidad = dnorm(x, 5, 1)) 
```

```{r, fig.width=7, fig.height=3}
# qnorm es la funci칩n de cuantiles de una normal
cuantil_50 <- qnorm(0.50, 5, 1)
cuantil_90 <- qnorm(0.95, 5, 1)
# graficamos
densidad_tbl <- densidad_tbl |> 
  mutate(menor_50 = x >= cuantil_50) |> 
  mutate(menor_90 = x >= cuantil_90)
g_normal_50 <- ggplot(densidad_tbl, aes(y = densidad)) + 
  ylab('f(x)') + 
  geom_area(aes(x = x, fill = menor_50)) + 
  geom_line(aes(x = x), alpha = 0.1) +
  geom_vline(xintercept = cuantil_50) + theme(legend.position = "none") +
  annotate("text", 4.3, 0.2, label = "50%") +
  labs(subtitle = paste0("q(0.5)=", round(cuantil_50,1)))
g_normal_90 <- ggplot(densidad_tbl, aes(y = densidad)) + 
  ylab('f(x)') + 
  geom_area(aes(x = x, fill = menor_90)) + 
  geom_line(aes(x = x), alpha = 0.1) +
  geom_vline(xintercept = cuantil_90) + theme(legend.position = "none") +
  annotate("text", 5.0, 0.2, label = "95%") +
  labs(subtitle = paste0("q(0.95)=", round(cuantil_90,1)))
g_normal_50 + g_normal_90
```

Como todas las distribuciones normales tienen la misma forma, y para obtener una de otra
solo basta reescalar y desplazar, para calcular los cuantiles de una 
variable con distribuci칩n normal $\mathsf{N}(\mu, \sigma)$
s칩lo tenemos que saber los cuantiles de la distribuci칩n normal est치ndar $\mathsf{N}(0,1)$ y escalarlos
apropiadamente por su media y desviaci칩n est치ndar

$$q(f, \mu, \sigma) = \mu + \sigma q(f, 0, 1)$$ 
Puedes demostrar esto sin mucha dificultad empezando con $P(X\leq q) = f$ y estandarizando:

$$P(X\leq q(f, \mu, \sigma)) = f \implies P\left (Z\leq \frac{q(f,\mu,\sigma) - \mu}{\sigma}\right)=f$$
y esto implica que
$$q(f, 0, 1) =  \frac{q(f,\mu,\sigma) - \mu}{\sigma} \implies q(f, \mu, \sigma) = \mu + \sigma q(f, 0, 1)$$

De modo que si grafic치ramos los cuantiles de una distribuci칩n $\mathsf{N}(\mu, \sigma)$ contra
los cuantiles de una distribuci칩n $\mathsf{N}(0,1)$, estos cuantiles aparecen en una l칤nea recta:

```{r, fig.width=4,fig.height=3}
comparacion_tbl <- tibble(f = seq(0.01, 0.99, 0.01)) |> 
  mutate(cuantiles_normal = qnorm(f, 5, 3),
         cuantiles_norm_estandar = qnorm(f, 0, 1))
ggplot(comparacion_tbl, aes(cuantiles_norm_estandar, cuantiles_normal)) + 
  geom_point()
```

Ahora supongamos que tenemos una muestra $X_1, \ldots, X_n$. 쮺칩mo podemos
checar si estos datos tienen una distribuci칩n aproximadamente normal? 

- Si la muestra tiene una distribuci칩n aproximadamente $\mathsf{N}(\mu, \sigma)$, entonces
sus cuantiles muestrales y los cuantiles respectivos de la normal est치ndar est치n aproximadamente
en una l칤nea recta.

Primero veamos un ejemplo donde los datos son generados seg칰n una normal. 

```{r, fig.width=7, fig.height=3}
set.seed(21)
muestra <- tibble(x_1 = rnorm(60, 10, 3), x_2 = rgamma(60, 2, 5))
graf_1 <- ggplot(muestra, aes(sample = x_1)) +
  geom_qq(distribution = stats::qnorm) +
  geom_qq_line(colour = "red")
graf_2 <- ggplot(muestra, aes(sample = x_2)) +
  geom_qq(distribution = stats::qnorm) +
  geom_qq_line(colour = "red")
graf_1 + graf_2
```
쮺u치les son los datos aproximadamente normales? 쮺칩mo interpretas las desviaciones de 
la segunda gr치fica en t칠rminos de la forma de la distribuci칩n normal?

## Prueba de hip칩tesis de normalidad {-}

Para interpretar las gr치ficas de cuantiles normales se requiere pr치ctica, 
pues claramente los datos, a칰n cuando provengan de una distribuci칩n normal, no
van a caer justo sobre una l칤nea recta y observaremos variabilidad. Esto no descarta
necesariamente que los datos sean aproximadamente normales. Con la pr치ctica, generalmente
esta gr치fica nos da una buena indicaci칩n si el supuesto de normalidad es apropiado.

Sin embargo, podemos hacer una prueba de hip칩tesis formal de normalidad si 
quisi칠ramos. La hip칩tesis nula es la siguiente:

- Los datos provienen de una distribuci칩n normal, y las desviaciones que observamos
de una l칤nea recta se deben a variaci칩n muestral.
- Podemos generar datos nulos tomando la media y desviaci칩n est치ndar muestrales, y
generando muestras normales $\mathsf{N}(\bar{x}, s)$.
- Usamos el *lineup*, produciendo datos bajo la hip칩tesis nula y viendo si
podemos distinguir los datos. Por ejemplo:

```{r, fig.height=7.2, fig.width=7}
library(nullabor)
lineup_normal <- lineup(null_dist("x_2", dist = "normal"), muestra)
ggplot(lineup_normal, aes(sample = x_2)) +
  geom_qq(distribution = stats::qnorm) +
  geom_qq_line(colour = "red") +
  facet_wrap(~ .sample)
```

En esta gr치fica claramente rechazar칤amos la hip칩tesis de normalidad. Sin embargo, para
la primera muestra, obtenemos:

```{r, fig.height=7.2, fig.width=7}
lineup_normal <- lineup(null_dist("x_1", dist = "normal"), muestra)
ggplot(lineup_normal, aes(sample = x_1)) +
  geom_qq(distribution = stats::qnorm) +
  geom_qq_line(colour = "red") +
  facet_wrap(~ .sample)
```
Los datos verdaderos est치n en

```{r}
attr(lineup_normal, "pos")
```

### Ejemplo {-}

Consideremos el problema de estimar el total poblacional de los precios de las casas
que se vendieron. El estimador que usamos fue la suma muestral expandida por un 
factor. Vamos a checar qu칠 tan cerca de la normalidad est치 la distribuci칩n de
meustreo de esta estad칤stica ($n=250$):

```{r}
replicaciones_2
```


```{r, fig.width = 4, fig.height=3}
ggplot(replicaciones_2, aes(sample = est_total_millones)) +
  geom_qq(alpha = 0.3) + 
  geom_qq_line(colour = "red")
```

```{r, echo=FALSE, eval=FALSE}
lineup_normal <- lineup(null_dist("est_total_millones", dist = "normal"), replicaciones_2)
ggplot(lineup_normal, aes(sample = est_total_millones)) +
  geom_qq(distribution = stats::qnorm) +
  geom_qq_line(colour = "red") +
  facet_wrap(~ .sample)
```


Y vemos que en efecto el TCL aplica en este ejemplo, y la aproximaci칩n es buena.
Aunque la poblaci칩n original es sesgada, la descripci칩n de la distribuci칩n de
muestreo es sorprendemente compacta:

- La distribuci칩n de muestreo de nuestro estimador del total $\hat{t}$ es
aproximadamente normal con media $\bar{x}$ y desviaci칩n est치ndar $s$, donde:

```{r}
mu <- mean(replicaciones_2$est_total_millones)
s <- sd(replicaciones_2$est_total_millones)
c(mu = mu, s = s) |> round(2)
```
Estas cantidades est치n en millones de d칩lares.

### Ejemplo {-}

Supongamos que queremos calcular la probabilidad que la suma de 30 variables
uniformes en $[0,1]$ independientes sea mayor que 18. Podr칤amos aproximar esta
cantidad usando simulaci칩n. Otra manera de aproximar esta cantidad es con
el TCL, de la siguiente forma:

Si $S=X_1 + X_2 + X_{30}$, entonces la media de $S$ es 15 (쯖칩mo se calcula?)
y su desviaci칩n est치ndar es $\sqrt{\frac{30}{12}}$. La suma es entonces
aproximadamente $\mathsf{N}\left(15, \sqrt{\frac{30}{12}}\right)$. Entonces

$$P(S > 18) = P \left (\frac{S - 15}{\sqrt{\frac{30}{12}}}  > \frac{18 - 15}{\sqrt{\frac{30}{12}}}\right) \approx P(Z > 1.897)$$

donde $Z$ es normal est치ndar. Esta 칰ltima cantidad la calculamos
usando la funci칩n de distribuci칩n de la normal est치ndar, y nuestra aproximaci칩n es

```{r}
1 - pnorm(1.897)
```

Podemos checar nuestro c치lculo usando simulaci칩n:

```{r}
tibble(n_sim = 1:100000) |> 
  mutate(suma = map_dbl(n_sim, ~ sum(runif(30)))) |> 
  summarise(prob_may_18 = mean(suma > 18), .groups = "drop")
```
Y vemos que la aproximaci칩n normal es buena para fines pr치cticos.

```{block, type='ejercicio'}
Usando simulaciones haz un histograma que aproxime la distribuci칩n de muestreo de $S$. 
Haz una gr치fica de cuantiles normales para checar la normalidad de esta distribuci칩n.
```

## Ejemplo {-}
Cuando el sesgo de la distribuci칩n poblacional es grande, puede ser necesario
que $n$ sea muy grande para que la aproximaci칩n normal sea aceptable para el 
promedio o la suma. Por ejemplo, si tomamos una gamma con par치metro de forma chico,
$n = 30$ no es suficientemente bueno, especialmente si quisi칠ramos 
aproximar probabilidades en las colas de la distribuci칩n:


```{r}
sims_gamma <- map_df(1:2000, ~ tibble(suma = sum(rgamma(30, 0.1, 1))), 
                     .id = "n_sim")
ggplot(sims_gamma, aes(x = suma)) + geom_histogram()
```

## M치s del Teorema central del l칤mite {-}

- El teorema central del l칤mite aplica a situaciones m치s generales que
las del enunciado del teorema b치sico. Por ejemplo, 
  + aplica a poblaciones finitas (como vimos en el ejemplo de las casas), en 1960
  Jaroslav Hajek demostr칩 una versi칩n del TCL bajo muestreo sin
reemplazo.
  + Mas all치 de la media muestral, el TCL se puede utilizar para m치s estad칤sticas ya que muchas
 pueden verse como promedios, como totales o errores est치ndar. El TLC se ha generalizado incluso para cuantiles muestrales.
 
 <!-- Loosely speaking, a consequence of the CLT for sample quantiles is that the 100洧녷% sample quantile of a large number of identically distributed random variables, each with probability density function 洧녭 -->
 <!-- and 100洧녷% quantile 洧랠(洧녷), has approximately a normal distribution. See, for example, Lehmann (1999) for a precise statement and conditions. -->

- Es importante notar que la calidad de la aproximaci칩n del TCL depende de caracter칤sticas
de la poblaci칩n y tambi칠n del tama침o de muestra $n$. Para ver si el TCL aplica, podemos hacer ejercicios de simulaci칩n bajo diferentes supuestos acerca de la poblaci칩n. 
Tambi칠n veremos m치s adelante, con remuestreo, maneras de checar si es factible el TCL dependiendo del an치lisis de una muestra dada que tengamos.

- El TCL era particularmente importante en la pr치ctica antes de que pudi칠ramos
hacer simulaci칩n por computadora. Era la 칰nica manera de aproximar y entender la distribuci칩n muestral fuera de c치lculos anal칤ticos (como los que hicimos para
el m치ximo de un conjunto de uniformes, por ejemplo). 

- Hoy en d칤a, veremos que podemos hacer simulaci칩n para obtener respuestas m치s
exactas, particularmente en la construcci칩n de intervalos de confianza, por ejemplo. Dependemos menos de **resultados asint칩ticos**, como el TCL.

- Cuando aproximamos una distribuci칩n discreta mediante la distribuci칩n normal,
conviene hacer *correcciones de continuidad*, como se explica en [@Chihara], 4.3.2.

<!-- Hayek: https://www.kybernetika.cz/content/1995/3/251/paper.pdf -->
<!-- Lehman: https://www.datascienceassn.org/sites/default/files/Elements%20of%20Large-Sample%20Theory%20-%20Lehmann.pdf -->






